{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Moda2CVlvo3E",
        "outputId": "a3150dcb-2831-4655-cf41-3f685253808c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using sample dataset, shape: (12, 5)\n",
            "Built TF-IDF matrix: (12, 250)\n",
            "Computed cosine similarity matrix\n",
            "Saved model artifacts to /content\n",
            "\n",
            "Query: Classic Margherita Pizza\n",
            " - Penne alla Vodka (cuisine: Italian) — score: 0.035\n",
            " - Green Curry with Tofu (cuisine: Thai) — score: 0.033\n",
            " - Vegetable Stir Fry (cuisine: Chinese) — score: 0.031\n",
            " - Chana Masala (cuisine: Indian) — score: 0.026\n",
            " - Shrimp Scampi (cuisine: Italian) — score: 0.023\n",
            "\n",
            "Query: Chana Masala\n",
            " - Spicy Chicken Tikka Masala (cuisine: Indian) — score: 0.192\n",
            " - Falafel Wrap (cuisine: Mediterranean) — score: 0.075\n",
            " - Beef Tacos (cuisine: Mexican) — score: 0.066\n",
            " - Classic Margherita Pizza (cuisine: Italian) — score: 0.026\n",
            " - Penne alla Vodka (cuisine: Italian) — score: 0.016\n",
            "\n",
            "Query: Penne alla Vodka\n",
            " - Shrimp Scampi (cuisine: Italian) — score: 0.076\n",
            " - Spicy Chicken Tikka Masala (cuisine: Indian) — score: 0.050\n",
            " - Classic Margherita Pizza (cuisine: Italian) — score: 0.035\n",
            " - Falafel Wrap (cuisine: Mediterranean) — score: 0.017\n",
            " - Chana Masala (cuisine: Indian) — score: 0.016\n",
            "\n",
            "Query: Pad Thai\n",
            " - Shrimp Scampi (cuisine: Italian) — score: 0.057\n",
            " - Black Bean Burrito (cuisine: Mexican) — score: 0.051\n",
            " - Green Curry with Tofu (cuisine: Thai) — score: 0.045\n",
            " - Vegetable Stir Fry (cuisine: Chinese) — score: 0.021\n",
            " - Classic Margherita Pizza (cuisine: Italian) — score: 0.000\n",
            "\n",
            "Saved sample recommendation summary to /content/recommendation_summary.csv\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Content-based Recipe Recommender — Colab-ready script\n",
        "\n",
        "This is content-based filtering — not using ratings nor collaborative signals.\n",
        "\"\"\"\n",
        "# If running in Colab you can optionally mount Drive:\n",
        "# from google.colab import drive\n",
        "\n",
        "import re\n",
        "import pickle\n",
        "from typing import List, Optional, Dict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import os\n",
        "\n",
        "# -----------------------\n",
        "# Configuration / Helpers\n",
        "# -----------------------\n",
        "TFIDF_PICKLE = \"tfidf_matrix.pkl\"\n",
        "VECTORIZER_PICKLE = \"tfidf_vectorizer.pkl\"\n",
        "COSINE_PICKLE = \"cosine_sim.pkl\"\n",
        "INDICES_PICKLE = \"indices.pkl\"\n",
        "\n",
        "def clean_text(s: Optional[str]) -> str:\n",
        "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
        "        return \"\"\n",
        "    s = str(s).lower()\n",
        "    # keep alphanumerics, commas, hyphens, hashes (for tags), spaces\n",
        "    s = re.sub(r\"[^a-z0-9 ,#-]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def make_soup(row: pd.Series, fields: List[str] = None) -> str:\n",
        "    # fields order matters if you want to weight them manually later\n",
        "    if fields is None:\n",
        "        fields = [\"title\", \"ingredients\", \"tags\", \"cuisine\"]\n",
        "    parts = []\n",
        "    for f in fields:\n",
        "        if f in row:\n",
        "            parts.append(clean_text(row.get(f, \"\")))\n",
        "    # join with spaces\n",
        "    return \" \".join([p for p in parts if p])\n",
        "\n",
        "# -----------------------\n",
        "# Core pipeline functions\n",
        "# -----------------------\n",
        "def build_or_load_model(df: pd.DataFrame,\n",
        "                        soup_fields: List[str] = None,\n",
        "                        force_rebuild: bool = False,\n",
        "                        persist_dir: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Build TF-IDF matrix, cosine similarity matrix and indices.\n",
        "    If files exist in persist_dir and force_rebuild is False, they will be loaded.\n",
        "    Returns: (tfidf_matrix, vectorizer, cosine_sim, indices)\n",
        "    \"\"\"\n",
        "    if persist_dir is None:\n",
        "        persist_dir = os.getcwd()\n",
        "\n",
        "    tfidf_path = os.path.join(persist_dir, TFIDF_PICKLE)\n",
        "    vectorizer_path = os.path.join(persist_dir, VECTORIZER_PICKLE)\n",
        "    cosine_path = os.path.join(persist_dir, COSINE_PICKLE)\n",
        "    indices_path = os.path.join(persist_dir, INDICES_PICKLE)\n",
        "\n",
        "    # Create 'soup'\n",
        "    df = df.copy()\n",
        "    df['soup'] = df.apply(lambda r: make_soup(r, fields=soup_fields), axis=1)\n",
        "\n",
        "    # Try loading precomputed artifacts\n",
        "    if (not force_rebuild) and os.path.exists(tfidf_path) and os.path.exists(vectorizer_path) and os.path.exists(cosine_path) and os.path.exists(indices_path):\n",
        "        try:\n",
        "            with open(vectorizer_path, \"rb\") as f:\n",
        "                vectorizer = pickle.load(f)\n",
        "            with open(tfidf_path, \"rb\") as f:\n",
        "                tfidf_matrix = pickle.load(f)\n",
        "            with open(cosine_path, \"rb\") as f:\n",
        "                cosine_sim = pickle.load(f)\n",
        "            with open(indices_path, \"rb\") as f:\n",
        "                indices = pickle.load(f)\n",
        "            print(\"Loaded precomputed model artifacts from\", persist_dir)\n",
        "            return tfidf_matrix, vectorizer, cosine_sim, indices, df\n",
        "        except Exception as e:\n",
        "            print(\"Failed loading artifacts, rebuilding. Reason:\", e)\n",
        "\n",
        "    # Vectorize soups\n",
        "    # Using uni- and bi-grams often helps capture short phrases like 'garlic butter'\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', max_features=20000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(df['soup'].values)\n",
        "    print(\"Built TF-IDF matrix:\", tfidf_matrix.shape)\n",
        "\n",
        "    # Compute cosine similarity (linear_kernel faster with TF-IDF)\n",
        "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "    print(\"Computed cosine similarity matrix\")\n",
        "\n",
        "    # Build indices map (lowercased title -> index), drop duplicates keeping first\n",
        "    indices = pd.Series(df.index, index=df['title'].str.lower()).drop_duplicates()\n",
        "\n",
        "    # Persist artifacts\n",
        "    try:\n",
        "        with open(vectorizer_path, \"wb\") as f:\n",
        "            pickle.dump(vectorizer, f)\n",
        "        with open(tfidf_path, \"wb\") as f:\n",
        "            pickle.dump(tfidf_matrix, f)\n",
        "        with open(cosine_path, \"wb\") as f:\n",
        "            pickle.dump(cosine_sim, f)\n",
        "        with open(indices_path, \"wb\") as f:\n",
        "            pickle.dump(indices, f)\n",
        "        print(\"Saved model artifacts to\", persist_dir)\n",
        "    except Exception as e:\n",
        "        print(\"Warning: failed to persist artifacts:\", e)\n",
        "\n",
        "    return tfidf_matrix, vectorizer, cosine_sim, indices, df\n",
        "\n",
        "def get_recommendations(title: str,\n",
        "                        df: pd.DataFrame,\n",
        "                        cosine_sim: np.ndarray,\n",
        "                        indices: pd.Series,\n",
        "                        topn: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Return topn similar recipes for a given recipe title.\n",
        "    Output: list of dicts with keys: id (if exists), title, cuisine (if exists), score\n",
        "    \"\"\"\n",
        "    title_key = title.lower()\n",
        "    if title_key not in indices:\n",
        "        raise ValueError(f\"Title '{title}' not found in dataset. Available sample titles:\\n{', '.join(df['title'].head(20).tolist())}\")\n",
        "    idx = indices[title_key]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = [(i, s) for i, s in sim_scores if i != idx]\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    top = sim_scores[:topn]\n",
        "    results = []\n",
        "    for i, score in top:\n",
        "        entry = {\n",
        "            \"index\": int(i),\n",
        "            \"score\": float(score),\n",
        "            \"title\": str(df.at[i, 'title'])\n",
        "        }\n",
        "        if 'id' in df.columns:\n",
        "            try:\n",
        "                entry['id'] = int(df.at[i, 'id'])\n",
        "            except Exception:\n",
        "                entry['id'] = df.at[i, 'id']\n",
        "        if 'cuisine' in df.columns:\n",
        "            entry['cuisine'] = df.at[i, 'cuisine']\n",
        "        results.append(entry)\n",
        "    return results\n",
        "\n",
        "# -----------------------\n",
        "# Example / CLI usage\n",
        "# -----------------------\n",
        "SAMPLE_DATA = [\n",
        "    {\"id\": 1, \"title\": \"Classic Margherita Pizza\", \"ingredients\": \"flour, water, yeast, tomato, mozzarella, basil, olive oil, salt\", \"cuisine\": \"Italian\", \"tags\": \"vegetarian, pizza, baked\"},\n",
        "    {\"id\": 2, \"title\": \"Spicy Chicken Tikka Masala\", \"ingredients\": \"chicken, yogurt, garam masala, tomato, ginger, garlic, cream\", \"cuisine\": \"Indian\", \"tags\": \"spicy, curry, chicken\"},\n",
        "    {\"id\": 3, \"title\": \"Vegetable Stir Fry\", \"ingredients\": \"broccoli, bell pepper, carrot, soy sauce, garlic, sesame oil\", \"cuisine\": \"Chinese\", \"tags\": \"vegetarian, quick, stir-fry\"},\n",
        "    {\"id\": 4, \"title\": \"Beef Tacos\", \"ingredients\": \"beef, taco shells, lettuce, tomato, cheddar, onion, cumin\", \"cuisine\": \"Mexican\", \"tags\": \"handheld, beef\"},\n",
        "    {\"id\": 5, \"title\": \"Penne alla Vodka\", \"ingredients\": \"penne, tomato, cream, vodka, parmesan, garlic\", \"cuisine\": \"Italian\", \"tags\": \"pasta, creamy\"},\n",
        "    {\"id\": 6, \"title\": \"Pad Thai\", \"ingredients\": \"rice noodles, shrimp, tamarind, fish sauce, egg, bean sprouts, peanuts\", \"cuisine\": \"Thai\", \"tags\": \"noodles, sweet-sour\"},\n",
        "    {\"id\": 7, \"title\": \"Chana Masala\", \"ingredients\": \"chickpeas, tomato, onion, garam masala, cumin, coriander\", \"cuisine\": \"Indian\", \"tags\": \"vegetarian, protein-rich\"},\n",
        "    {\"id\": 8, \"title\": \"Grilled Cheese Sandwich\", \"ingredients\": \"bread, cheddar, butter\", \"cuisine\": \"American\", \"tags\": \"quick, comfort\"},\n",
        "    {\"id\": 9, \"title\": \"Falafel Wrap\", \"ingredients\": \"chickpeas, parsley, cumin, garlic, pita, tahini\", \"cuisine\": \"Mediterranean\", \"tags\": \"vegetarian, street-food\"},\n",
        "    {\"id\": 10, \"title\": \"Shrimp Scampi\", \"ingredients\": \"shrimp, garlic, butter, lemon, parsley, linguine\", \"cuisine\": \"Italian\", \"tags\": \"seafood, pasta\"},\n",
        "    {\"id\": 11, \"title\": \"Black Bean Burrito\", \"ingredients\": \"black beans, rice, tortilla, avocado, salsa, cheese\", \"cuisine\": \"Mexican\", \"tags\": \"vegetarian, handheld\"},\n",
        "    {\"id\": 12, \"title\": \"Green Curry with Tofu\", \"ingredients\": \"tofu, coconut milk, green curry paste, basil, eggplant\", \"cuisine\": \"Thai\", \"tags\": \"vegetarian, spicy\"}\n",
        "]\n",
        "\n",
        "def load_data_from_csv(csv_path: Optional[str] = None) -> pd.DataFrame:\n",
        "    if csv_path:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"Loaded CSV from {csv_path}, shape: {df.shape}\")\n",
        "    else:\n",
        "        df = pd.DataFrame(SAMPLE_DATA)\n",
        "        print(\"Using sample dataset, shape:\", df.shape)\n",
        "    # ensure 'title' exists\n",
        "    if 'title' not in df.columns:\n",
        "        raise ValueError(\"Data must contain a 'title' column.\")\n",
        "    # fillna for common columns to avoid errors\n",
        "    for col in ['ingredients', 'tags', 'cuisine']:\n",
        "        if col not in df.columns:\n",
        "            df[col] = \"\"\n",
        "        else:\n",
        "            df[col] = df[col].fillna(\"\")\n",
        "    # If id doesn't exist, create a numeric id\n",
        "    if 'id' not in df.columns:\n",
        "        df['id'] = range(1, len(df) + 1)\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: in Colab set CSV_PATH to your file or leave None to use sample\n",
        "    CSV_PATH = None  # e.g. '/content/drive/MyDrive/recipes.csv'\n",
        "    df = load_data_from_csv(CSV_PATH)\n",
        "\n",
        "    # Build model (set force_rebuild=True to skip loading any persisted artifacts)\n",
        "    tfidf_matrix, vectorizer, cosine_sim, indices, df = build_or_load_model(df, soup_fields=None, force_rebuild=True, persist_dir=\"/content\")\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"Classic Margherita Pizza\",\n",
        "        \"Chana Masala\",\n",
        "        \"Penne alla Vodka\",\n",
        "        \"Pad Thai\",\n",
        "    ]\n",
        "\n",
        "    for q in queries:\n",
        "        print(\"\\nQuery:\", q)\n",
        "        try:\n",
        "            recs = get_recommendations(q, df=df, cosine_sim=cosine_sim, indices=indices, topn=5)\n",
        "            for r in recs:\n",
        "                cuisine = r.get('cuisine', 'N/A')\n",
        "                print(f\" - {r['title']} (cuisine: {cuisine}) — score: {r['score']:.3f}\")\n",
        "        except Exception as e:\n",
        "            print(\"  Error:\", e)\n",
        "\n",
        "    # Save a small CSV of the top recommendation for each item (example)\n",
        "    summary_rows = []\n",
        "    for idx, row in df.iterrows():\n",
        "        try:\n",
        "            recs = get_recommendations(row['title'], df=df, cosine_sim=cosine_sim, indices=indices, topn=1)\n",
        "            best = recs[0] if recs else {}\n",
        "            summary_rows.append({\n",
        "                \"source_id\": row['id'],\n",
        "                \"source_title\": row['title'],\n",
        "                \"rec_title\": best.get('title', ''),\n",
        "                \"rec_id\": best.get('id', ''),\n",
        "                \"score\": best.get('score', 0.0)\n",
        "            })\n",
        "        except Exception:\n",
        "            continue\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    outpath = \"/content/recommendation_summary.csv\"\n",
        "    summary_df.to_csv(outpath, index=False)\n",
        "    print(\"\\nSaved sample recommendation summary to\", outpath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Pq9mxXlvxSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}